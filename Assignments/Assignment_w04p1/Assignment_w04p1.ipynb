{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test =pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 9)\n",
      "(532, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train = train.drop(train[(train.BMI == 0)].index)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Diabetes\n",
    "x = train.drop('Diabetes', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)\n",
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((x.shape[1], 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.FloatTensor(x)\n",
    "y_tensor = torch.FloatTensor(y.to_numpy())\n",
    "test_tensor = torch.FloatTensor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10000 Cost: 0.693147\n",
      "Epoch  100/10000 Cost: 0.679658\n",
      "Epoch  200/10000 Cost: 0.667246\n",
      "Epoch  300/10000 Cost: 0.655816\n",
      "Epoch  400/10000 Cost: 0.645277\n",
      "Epoch  500/10000 Cost: 0.635547\n",
      "Epoch  600/10000 Cost: 0.626550\n",
      "Epoch  700/10000 Cost: 0.618218\n",
      "Epoch  800/10000 Cost: 0.610492\n",
      "Epoch  900/10000 Cost: 0.603313\n",
      "Epoch 1000/10000 Cost: 0.596635\n",
      "Epoch 1100/10000 Cost: 0.590410\n",
      "Epoch 1200/10000 Cost: 0.584599\n",
      "Epoch 1300/10000 Cost: 0.579166\n",
      "Epoch 1400/10000 Cost: 0.574079\n",
      "Epoch 1500/10000 Cost: 0.569308\n",
      "Epoch 1600/10000 Cost: 0.564827\n",
      "Epoch 1700/10000 Cost: 0.560612\n",
      "Epoch 1800/10000 Cost: 0.556643\n",
      "Epoch 1900/10000 Cost: 0.552898\n",
      "Epoch 2000/10000 Cost: 0.549362\n",
      "Epoch 2100/10000 Cost: 0.546018\n",
      "Epoch 2200/10000 Cost: 0.542853\n",
      "Epoch 2300/10000 Cost: 0.539852\n",
      "Epoch 2400/10000 Cost: 0.537005\n",
      "Epoch 2500/10000 Cost: 0.534300\n",
      "Epoch 2600/10000 Cost: 0.531727\n",
      "Epoch 2700/10000 Cost: 0.529279\n",
      "Epoch 2800/10000 Cost: 0.526945\n",
      "Epoch 2900/10000 Cost: 0.524720\n",
      "Epoch 3000/10000 Cost: 0.522596\n",
      "Epoch 3100/10000 Cost: 0.520566\n",
      "Epoch 3200/10000 Cost: 0.518626\n",
      "Epoch 3300/10000 Cost: 0.516769\n",
      "Epoch 3400/10000 Cost: 0.514990\n",
      "Epoch 3500/10000 Cost: 0.513285\n",
      "Epoch 3600/10000 Cost: 0.511650\n",
      "Epoch 3700/10000 Cost: 0.510081\n",
      "Epoch 3800/10000 Cost: 0.508574\n",
      "Epoch 3900/10000 Cost: 0.507126\n",
      "Epoch 4000/10000 Cost: 0.505733\n",
      "Epoch 4100/10000 Cost: 0.504393\n",
      "Epoch 4200/10000 Cost: 0.503103\n",
      "Epoch 4300/10000 Cost: 0.501861\n",
      "Epoch 4400/10000 Cost: 0.500663\n",
      "Epoch 4500/10000 Cost: 0.499509\n",
      "Epoch 4600/10000 Cost: 0.498395\n",
      "Epoch 4700/10000 Cost: 0.497319\n",
      "Epoch 4800/10000 Cost: 0.496281\n",
      "Epoch 4900/10000 Cost: 0.495279\n",
      "Epoch 5000/10000 Cost: 0.494309\n",
      "Epoch 5100/10000 Cost: 0.493372\n",
      "Epoch 5200/10000 Cost: 0.492466\n",
      "Epoch 5300/10000 Cost: 0.491589\n",
      "Epoch 5400/10000 Cost: 0.490740\n",
      "Epoch 5500/10000 Cost: 0.489918\n",
      "Epoch 5600/10000 Cost: 0.489121\n",
      "Epoch 5700/10000 Cost: 0.488350\n",
      "Epoch 5800/10000 Cost: 0.487602\n",
      "Epoch 5900/10000 Cost: 0.486876\n",
      "Epoch 6000/10000 Cost: 0.486173\n",
      "Epoch 6100/10000 Cost: 0.485490\n",
      "Epoch 6200/10000 Cost: 0.484828\n",
      "Epoch 6300/10000 Cost: 0.484185\n",
      "Epoch 6400/10000 Cost: 0.483560\n",
      "Epoch 6500/10000 Cost: 0.482954\n",
      "Epoch 6600/10000 Cost: 0.482364\n",
      "Epoch 6700/10000 Cost: 0.481792\n",
      "Epoch 6800/10000 Cost: 0.481235\n",
      "Epoch 6900/10000 Cost: 0.480694\n",
      "Epoch 7000/10000 Cost: 0.480167\n",
      "Epoch 7100/10000 Cost: 0.479655\n",
      "Epoch 7200/10000 Cost: 0.479157\n",
      "Epoch 7300/10000 Cost: 0.478672\n",
      "Epoch 7400/10000 Cost: 0.478200\n",
      "Epoch 7500/10000 Cost: 0.477741\n",
      "Epoch 7600/10000 Cost: 0.477293\n",
      "Epoch 7700/10000 Cost: 0.476858\n",
      "Epoch 7800/10000 Cost: 0.476433\n",
      "Epoch 7900/10000 Cost: 0.476020\n",
      "Epoch 8000/10000 Cost: 0.475617\n",
      "Epoch 8100/10000 Cost: 0.475224\n",
      "Epoch 8200/10000 Cost: 0.474841\n",
      "Epoch 8300/10000 Cost: 0.474467\n",
      "Epoch 8400/10000 Cost: 0.474103\n",
      "Epoch 8500/10000 Cost: 0.473748\n",
      "Epoch 8600/10000 Cost: 0.473402\n",
      "Epoch 8700/10000 Cost: 0.473064\n",
      "Epoch 8800/10000 Cost: 0.472734\n",
      "Epoch 8900/10000 Cost: 0.472412\n",
      "Epoch 9000/10000 Cost: 0.472098\n",
      "Epoch 9100/10000 Cost: 0.471791\n",
      "Epoch 9200/10000 Cost: 0.471492\n",
      "Epoch 9300/10000 Cost: 0.471200\n",
      "Epoch 9400/10000 Cost: 0.470914\n",
      "Epoch 9500/10000 Cost: 0.470635\n",
      "Epoch 9600/10000 Cost: 0.470363\n",
      "Epoch 9700/10000 Cost: 0.470097\n",
      "Epoch 9800/10000 Cost: 0.469837\n",
      "Epoch 9900/10000 Cost: 0.469583\n",
      "Epoch 10000/10000 Cost: 0.469334\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.001)\n",
    "\n",
    "nb_epochs = 10000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_tensor.matmul(W) + b) # or .mm or @\n",
    "\n",
    "\n",
    "    ## 중간 생략 \n",
    "\n",
    "    cost = loss(hypothesis, y_tensor.unsqueeze(1))\n",
    "\n",
    "    # cost = -(y_tensor * torch.log(hypothesis) + \n",
    "            #  (1 - y_tensor) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6127],\n",
       "        [-1.2473],\n",
       "        [-1.6393],\n",
       "        [-1.1678],\n",
       "        [ 0.0551],\n",
       "        [ 0.0721],\n",
       "        [-3.3080],\n",
       "        [-0.0300],\n",
       "        [ 0.1619],\n",
       "        [ 1.0093],\n",
       "        [-0.9088],\n",
       "        [ 1.6611],\n",
       "        [-0.2424],\n",
       "        [-0.5388],\n",
       "        [-2.0372],\n",
       "        [-0.4655],\n",
       "        [-1.4883],\n",
       "        [-1.9251],\n",
       "        [ 0.8048],\n",
       "        [ 0.2024],\n",
       "        [-0.9713],\n",
       "        [-1.8372],\n",
       "        [-0.0211],\n",
       "        [-1.7617],\n",
       "        [ 0.2723],\n",
       "        [ 1.4523],\n",
       "        [-1.7050],\n",
       "        [-2.6517],\n",
       "        [-0.6549],\n",
       "        [-1.4345],\n",
       "        [ 1.6883],\n",
       "        [ 1.5223],\n",
       "        [ 1.1614],\n",
       "        [ 0.6259],\n",
       "        [ 0.5647],\n",
       "        [ 0.7590],\n",
       "        [ 2.0652],\n",
       "        [-0.8917],\n",
       "        [ 0.0686],\n",
       "        [ 0.2328],\n",
       "        [-2.0728],\n",
       "        [ 0.1333],\n",
       "        [ 0.2234],\n",
       "        [-0.4693],\n",
       "        [-2.5670],\n",
       "        [ 0.2187],\n",
       "        [ 0.6229],\n",
       "        [-1.0466],\n",
       "        [-0.3993],\n",
       "        [ 2.6736],\n",
       "        [-2.4223],\n",
       "        [ 0.7666],\n",
       "        [ 1.6163],\n",
       "        [-0.9283],\n",
       "        [-1.6320],\n",
       "        [-2.4453],\n",
       "        [ 0.9254],\n",
       "        [-4.5363],\n",
       "        [-0.2913],\n",
       "        [ 0.7604],\n",
       "        [ 0.6963],\n",
       "        [-0.6158],\n",
       "        [-0.4515],\n",
       "        [-0.6190],\n",
       "        [-1.7296],\n",
       "        [ 0.2032],\n",
       "        [-2.4092],\n",
       "        [ 1.0715],\n",
       "        [-2.5171],\n",
       "        [ 1.3629],\n",
       "        [ 0.8960],\n",
       "        [-1.9173],\n",
       "        [-1.1065],\n",
       "        [-1.5117],\n",
       "        [-1.7951],\n",
       "        [ 0.2391],\n",
       "        [-1.2225],\n",
       "        [-1.4770],\n",
       "        [-1.3501],\n",
       "        [-0.7343],\n",
       "        [ 0.4109],\n",
       "        [-1.4797],\n",
       "        [-2.1204],\n",
       "        [-0.2244],\n",
       "        [-0.8239],\n",
       "        [ 1.3837],\n",
       "        [ 1.6494],\n",
       "        [-0.4713],\n",
       "        [-1.5816],\n",
       "        [-1.8789],\n",
       "        [-2.0935],\n",
       "        [-0.7738],\n",
       "        [-4.7748],\n",
       "        [-0.3533],\n",
       "        [ 0.0083],\n",
       "        [ 0.3299],\n",
       "        [-0.5012],\n",
       "        [-1.4952],\n",
       "        [ 0.8017],\n",
       "        [-1.6941],\n",
       "        [ 0.6696],\n",
       "        [-1.9881],\n",
       "        [ 1.1827],\n",
       "        [ 0.1694],\n",
       "        [ 0.4365],\n",
       "        [-0.7805],\n",
       "        [-0.9261],\n",
       "        [ 0.8824],\n",
       "        [-1.3793],\n",
       "        [ 0.0766],\n",
       "        [-1.8791],\n",
       "        [-0.5939],\n",
       "        [-3.5738],\n",
       "        [ 1.2015],\n",
       "        [-1.0409],\n",
       "        [-0.7753],\n",
       "        [ 0.5570],\n",
       "        [-1.0097],\n",
       "        [-2.1326],\n",
       "        [-0.2232],\n",
       "        [-2.2646],\n",
       "        [-0.7896],\n",
       "        [-0.9597],\n",
       "        [-1.9668],\n",
       "        [-0.9625],\n",
       "        [-0.0300],\n",
       "        [-3.0033],\n",
       "        [ 1.6345],\n",
       "        [ 2.2457],\n",
       "        [ 0.7968],\n",
       "        [ 0.5088],\n",
       "        [ 1.6396],\n",
       "        [-1.6795],\n",
       "        [-0.0754],\n",
       "        [ 1.3720],\n",
       "        [-1.5628],\n",
       "        [-1.1963],\n",
       "        [ 1.7098],\n",
       "        [ 1.1990],\n",
       "        [-3.4066],\n",
       "        [-1.9035],\n",
       "        [-2.5309],\n",
       "        [-1.0913],\n",
       "        [ 0.1284],\n",
       "        [-1.5746],\n",
       "        [-0.7089],\n",
       "        [-1.4296],\n",
       "        [-3.1263],\n",
       "        [-0.3171],\n",
       "        [ 0.9447],\n",
       "        [-1.6649],\n",
       "        [ 0.0548],\n",
       "        [-0.5914],\n",
       "        [-1.0716],\n",
       "        [-4.4607],\n",
       "        [-0.0754],\n",
       "        [-0.7884],\n",
       "        [ 0.2241],\n",
       "        [ 1.2424],\n",
       "        [-1.3837],\n",
       "        [-0.4300],\n",
       "        [ 0.5144],\n",
       "        [-1.1270],\n",
       "        [-3.2615],\n",
       "        [-1.5354],\n",
       "        [ 1.3447],\n",
       "        [-2.2708],\n",
       "        [-0.6430],\n",
       "        [ 0.9232],\n",
       "        [ 0.1371],\n",
       "        [ 0.2489],\n",
       "        [-1.1401],\n",
       "        [-0.6075],\n",
       "        [ 1.2036],\n",
       "        [ 0.9507],\n",
       "        [-1.6604],\n",
       "        [-0.6981],\n",
       "        [-0.5476],\n",
       "        [-0.7489],\n",
       "        [-0.5909],\n",
       "        [ 0.2307],\n",
       "        [ 0.3839],\n",
       "        [-0.2635],\n",
       "        [ 0.8653],\n",
       "        [ 0.6117],\n",
       "        [-1.7955],\n",
       "        [-2.2821],\n",
       "        [-1.4471],\n",
       "        [ 1.1774],\n",
       "        [-0.2054],\n",
       "        [-2.1105],\n",
       "        [-1.8648],\n",
       "        [ 1.2536],\n",
       "        [-0.5931],\n",
       "        [-1.7887],\n",
       "        [-2.0921],\n",
       "        [-4.2879],\n",
       "        [-2.1218],\n",
       "        [-0.9314],\n",
       "        [ 0.6039],\n",
       "        [-1.3135],\n",
       "        [-1.5700],\n",
       "        [-0.4271],\n",
       "        [-0.5591],\n",
       "        [ 0.6864],\n",
       "        [-1.6813],\n",
       "        [-1.7516],\n",
       "        [-0.9204],\n",
       "        [ 1.7908],\n",
       "        [ 0.5175],\n",
       "        [-0.6219],\n",
       "        [-1.0723],\n",
       "        [-1.4856],\n",
       "        [-1.6493],\n",
       "        [ 1.0119],\n",
       "        [-1.9150],\n",
       "        [ 1.6532],\n",
       "        [-0.6134],\n",
       "        [-0.5327],\n",
       "        [ 1.3838],\n",
       "        [ 0.2335],\n",
       "        [-1.4423],\n",
       "        [-2.0556],\n",
       "        [-1.3199],\n",
       "        [-2.0401],\n",
       "        [ 0.3717],\n",
       "        [-1.0625],\n",
       "        [-0.7320],\n",
       "        [-0.5282],\n",
       "        [-1.0986],\n",
       "        [-1.5474]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(test_tensor, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('submit.csv')\n",
    "submit.Diabetes = [int(x) for x in torch.sigmoid(test_tensor.matmul(W) + b).detach() > 0.6]\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b56a72e9acabfb7e93d1aa640bab3f729f4fa7900e427b4a015597563d3c661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
